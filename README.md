# KoInFoBench

KoInFoBench is a specialized evaluation dataset designed to assess the performance of Large Language Models (LLMs) on capabilities of Korean instructions following.

Inspired by [InFoBench](https://huggingface.co/datasets/kqsong/InFoBench) dataset, we extends their concpet by focusing on the nuances and features of Korean language.

This reporsitory is to provide the guidances 
  1. for generating responses from Proprietary LLM APIs
  2. for evaluating whether the respones are well-generated as following instructions

π“„ Paper is under writing and open soon! <br>
π¤— We welcome any opinions and contributions for our dataset [KoInFoBench](https://huggingface.co/datasets/kifai/KoInFoBench).

## Eavluation with KoInFoBench

### Setting Environments 

Download this github repository
```shell
git clone https://github.com/KIFAI/KoInFoBench.git
cd KoInFoBench
```

Create conda environment for this project and install python packages
```shell
conda create -n koinfobench python=3.10
conda activate koinfobench

pip install -r requirements.txt
```
Copy the file `.env.example` to `.env` and replace the values with your own API keys

```shell
OPENAI_API_KEY="YOUR OWN API KEY"
CLAUDE_API_KEY="YOUR OWN API KEY"
GEMINI_API_KEY="YOUR OWN API KEY"
HYPERCLOVAX_API_KEY="YOUR OWN API KEY"
HYPERCLOVAX_API_KEY_PRIMARY_EVAL="YOUR OWN API KEY"
```

### Step1. Check dataset usage

You can directly download it with huggingface datasets.

from datasets import load_dataset

dataset = load_dataset("kifai/KoInFoBench")


    
### 2. Generating responses of instructions

To generate responses from
- A specfic model
    ```shell
    python generation.py \
    --api_type openai  \
    --models gpt-4-0125-preview
    ```
- Multiple models (just append `--models` and `model_name`)
    ```shell
    python generation.py \
    --api_type openai \
    --models gpt-3.5-turbo-0125 \
    --models gpt-4-0125-preview \
    --models gpt-4-turbo-2024-04-09
    ```
- Multiple models of all api types
    ```shell
    python generation.py \
    --api_type all \
    --models gpt-3.5-turbo-0125 \
    --models gpt-4-0125-preview \
    --models claude-3-opus-20240229 \
    --models gemini-1.5-pro \
    --models hpx003
    ```

Generated responses are saved to the output_dir as `jsonl` file (e.g. `generation_{MODEL_NAME}.jsonl`). We share the generated responses using our own API budget in the directory `generations`. You can compare the result of various LLMs how they generate responses as Korean instructions.
```json
{"output": "Here is a comparison table of similar products based on price, rating, format, and paper size:\n\n| **Product** | **Price** | **Rating** | **Format** | **Paper Size** |\n| --- | --- | --- | --- | --- |\n| λ¨λ‹κΈ€λ΅λ¦¬ 3000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ PP μ¤ν”„λ§λ…ΈνΈ 16μ  μ¤„λ…ΈνΈ λ€ν•™ λ¨λ‹ν | β‚©25,000 | 4.5/5 | Ring-bound | A5 (148 x 210 mm) |\n| Morning Glory 2000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 3κ¶μ„ΈνΈ PP μ¤ν”„λ§λ…ΈνΈ 16μ  μ¤„λ…ΈνΈ λ€ν•™ λ¨λ‹ν | β‚©20,000 | 4.4/5 | Ring-bound | A5 (148 x 210 mm) |\n| Rhodia Webnotebook A5 96 sheets 80gsm | β‚©30,000 | 4.7/5 | Sewn-bound | A5 (148 x 210 mm) |\n| Quo Vadis Habana A5 128 pages 85gsm | β‚©35,000 | 4.6/5 | Sewn-bound | A5 (148 x 210 mm) |\n| Tsubame 3000 μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ PP μ¤ν”„λ§λ…ΈνΈ 16μ  μ¤„λ…ΈνΈ λ€ν•™ | β‚©22,000 | 4.3/5 | Ring-bound | A5 (148 x 210 mm) |\n\nNote: Prices may vary depending on the retailer and availability. Ratings are based on average customer reviews.", "input": "", "id": "1", "model_name": "meta-llama/Meta-Llama-3-70B-Instruct"}
{"output": "Here are the extracted option information from the product name:\n\n* Brand: MSI\n* Model: μ§€ν¬μ¤ RTX 4070 SUPER\n* Series: κ²μ΄λ° X\n* Form Factor: μ¬λ¦Ό\n* Memory Type: D6X\n* Capacity: 12GB\n* Cooler: νΈλΌμ΄ν”„λ΅μ Έ3\n\nLet me know if you need any further assistance! π", "input": "", "id": "2", "model_name": "meta-llama/Meta-Llama-3-70B-Instruct"}
...
```

### 3. Evaluating responses automatically using GPT-4
Before running the following scripts, make sure the generated files are placed on the directory `generation` or the directory you manually specified.
`gpt-4-0613` is the default evaluation model to judge the responses using decomposed questions. A instruction comprises multiple decomposed questions to judge whether a response is well-generated as following the instruction.

```shell
python evaluation.py \
--input_dir generation \
--eval_model gpt-4-0613 \
--seed 1234

# Model                           DRFR
# -----                           ----
# claude-3-opus-20240229         0.850
# hpx003                         0.738
# gpt-3.5-turbo-0125             0.734
# gpt-4-0125-preview             0.824
# gemini-1.5-pro                 0.811
# gpt-4-turbo-2024-04-09         0.880
```

Each data will include an "answers" key in the format of List[str] which composes of "Yes" or "No" answers to each decomposed question. The final output evaluation file will be saved in JSON format at location `{OUTPUT_DIR}/result.json`.

<details>
<summary>example of result.json</summary>
<pre markdown="1" code='json'>
{
  "claude-3-opus-20240229": {
    "filepath": "./generations_claude-3-opus-20240229.jsonl",
    "drfr": 0.8497854077253219,
    "evaluations": [
      {
        "input": "μƒν’: λ¨λ‹κΈ€λ΅λ¦¬ 3000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ PP μ¤ν”„λ§λ…ΈνΈ 16μ  μ¤„λ…ΈνΈ λ€ν•™ λ¨λ‹ν",
        "output": "| νΉμ§• | λ¨λ‹κΈ€λ΅λ¦¬ 3000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ | μ•„μ΄λ°”λΌκΈ° μ›λ§ PP λ…ΈνΈ 20μ  8mm μ¤„λ…ΈνΈ 5κ¶ | μ•„λ¥΄λ„λ³΄ μ λ³Έλ…ΈνΈ 16μ  μ¤„λ…ΈνΈ 10κ¶ |\n|:-|:-|:-|:-|\n| κ°€κ²© | 5,000μ› | 6,900μ› | 5,000μ› |\n| λ³„μ  | 4.5 | 4.0 | 4.5 |\n| λ‚΄μ§€ ν•νƒ | μ¤„λ…ΈνΈ | μ¤„λ…ΈνΈ | μ¤„λ…ΈνΈ |\n| μΆ…μ΄ μ‚¬μ΄μ¦ | 16μ  | 20μ  | 16μ  | \n| νΉμ§• | 4κ¶ μ„ΈνΈ, PP μ¤ν”„λ§ μ λ³Έ, λ€ν•™ λ¨λ‹ν μ¦μ • | 5κ¶ μ„ΈνΈ, PP μ¤ν”„λ§ μ λ³Έ, 8mm μ¤„κ°„κ²© | 10κ¶ μ„ΈνΈ, μ λ³Έλ…ΈνΈ |\n\nμ„ ν‘μ—μ„ μ μ‹λ μ„Έ κ°€μ§€ μƒν’μ€ λ¨λ‘ μ¤„λ…ΈνΈ ν•νƒμ λ…ΈνΈμ΄λ©°, 16μ  λλ” 20μ  ν¬κΈ°μ…λ‹λ‹¤. κ°€κ²©λ€λ„ 5,000μ›μ—μ„ 7,000μ› μ‚¬μ΄λ΅ μ μ‚¬ν•©λ‹λ‹¤. \n\nλ¨λ‹κΈ€λ΅λ¦¬ μƒν’μ€ 4κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , PP μ¤ν”„λ§ μ λ³Έ λ°©μ‹μ„ μ‚¬μ©ν•μ€μΌλ©°, λ€ν•™ λ¨λ‹νμ΄ μ¦μ •ν’μΌλ΅ μ κ³µλ©λ‹λ‹¤. \n\nμ•„μ΄λ°”λΌκΈ° μƒν’μ€ 5κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , μ—­μ‹ PP μ¤ν”„λ§ μ λ³Έ λ°©μ‹μ„ μ‚¬μ©ν•μ€μΌλ©°, 8mm μ¤„κ°„κ²©μ λ…ΈνΈμ…λ‹λ‹¤. \n\nμ•„λ¥΄λ„λ³΄ μƒν’μ€ 10κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , μ λ³Έλ…ΈνΈ ν•νƒμ…λ‹λ‹¤. \n\nλ”°λΌμ„ μ„ μ„Έ μƒν’μ€ λ¨λ‘ μ¤„λ…ΈνΈ ν•νƒμ— μ μ‚¬ν• ν¬κΈ°μ™€ κ°€κ²©λ€λ¥Ό κ°€μ§€κ³  μμ–΄ μ„λ΅ λΉ„κµ κ°€λ¥ν• μƒν’μΌλ΅ λ³Ό μ μμµλ‹λ‹¤.",
        "messages": [
          {
            "role": "user",
            "content": "Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice. Your selection should be based on your judgment as well as the following rules:\n\n- YES: Select 'YES' if the generated text entirely fulfills the condition specified in the question. However, note that even minor inaccuracies exclude the text from receiving a 'YES' rating. As an illustration. consider a question that asks. \"Does each sentence in the generated text use a second person?β€ If even one sentence does not use the second person, the answer should NOT be 'YES'. To qualify for a 'YES' rating, the generated text must be entirely accurate and relevant to the question\n\n- NO: Opt for 'NO' if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question. For instance, if the question asks. \"Is the second sentence in the generated text a compound sentence?\" and the generated text only has one sentence. it offers no relevant information to answer the question. Consequently, the answer should be 'NO'.'''\n\nInput:\n\"μƒν’: λ¨λ‹κΈ€λ΅λ¦¬ 3000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ PP μ¤ν”„λ§λ…ΈνΈ 16μ  μ¤„λ…ΈνΈ λ€ν•™ λ¨λ‹ν\"\n\nGenerated Text:\n\"| νΉμ§• | λ¨λ‹κΈ€λ΅λ¦¬ 3000 ν…λ§μ»¬λ¬ μ›λ§ λ…ΈνΈ 4κ¶μ„ΈνΈ | μ•„μ΄λ°”λΌκΈ° μ›λ§ PP λ…ΈνΈ 20μ  8mm μ¤„λ…ΈνΈ 5κ¶ | μ•„λ¥΄λ„λ³΄ μ λ³Έλ…ΈνΈ 16μ  μ¤„λ…ΈνΈ 10κ¶ |\n|:-|:-|:-|:-|\n| κ°€κ²© | 5,000μ› | 6,900μ› | 5,000μ› |\n| λ³„μ  | 4.5 | 4.0 | 4.5 |\n| λ‚΄μ§€ ν•νƒ | μ¤„λ…ΈνΈ | μ¤„λ…ΈνΈ | μ¤„λ…ΈνΈ |\n| μΆ…μ΄ μ‚¬μ΄μ¦ | 16μ  | 20μ  | 16μ  | \n| νΉμ§• | 4κ¶ μ„ΈνΈ, PP μ¤ν”„λ§ μ λ³Έ, λ€ν•™ λ¨λ‹ν μ¦μ • | 5κ¶ μ„ΈνΈ, PP μ¤ν”„λ§ μ λ³Έ, 8mm μ¤„κ°„κ²© | 10κ¶ μ„ΈνΈ, μ λ³Έλ…ΈνΈ |\n\nμ„ ν‘μ—μ„ μ μ‹λ μ„Έ κ°€μ§€ μƒν’μ€ λ¨λ‘ μ¤„λ…ΈνΈ ν•νƒμ λ…ΈνΈμ΄λ©°, 16μ  λλ” 20μ  ν¬κΈ°μ…λ‹λ‹¤. κ°€κ²©λ€λ„ 5,000μ›μ—μ„ 7,000μ› μ‚¬μ΄λ΅ μ μ‚¬ν•©λ‹λ‹¤. \n\nλ¨λ‹κΈ€λ΅λ¦¬ μƒν’μ€ 4κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , PP μ¤ν”„λ§ μ λ³Έ λ°©μ‹μ„ μ‚¬μ©ν•μ€μΌλ©°, λ€ν•™ λ¨λ‹νμ΄ μ¦μ •ν’μΌλ΅ μ κ³µλ©λ‹λ‹¤. \n\nμ•„μ΄λ°”λΌκΈ° μƒν’μ€ 5κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , μ—­μ‹ PP μ¤ν”„λ§ μ λ³Έ λ°©μ‹μ„ μ‚¬μ©ν•μ€μΌλ©°, 8mm μ¤„κ°„κ²©μ λ…ΈνΈμ…λ‹λ‹¤. \n\nμ•„λ¥΄λ„λ³΄ μƒν’μ€ 10κ¶ μ„ΈνΈλ΅ κµ¬μ„±λμ–΄ μκ³ , μ λ³Έλ…ΈνΈ ν•νƒμ…λ‹λ‹¤. \n\nλ”°λΌμ„ μ„ μ„Έ μƒν’μ€ λ¨λ‘ μ¤„λ…ΈνΈ ν•νƒμ— μ μ‚¬ν• ν¬κΈ°μ™€ κ°€κ²©λ€λ¥Ό κ°€μ§€κ³  μμ–΄ μ„λ΅ λΉ„κµ κ°€λ¥ν• μƒν’μΌλ΅ λ³Ό μ μμµλ‹λ‹¤.\"\n\nQuestion:\nμƒμ„±λ ν…μ¤νΈμ—λ” ν‘κ°€ μμµλ‹κΉ?\n"
          },
          {
            "role": "assistant",
            "content": "Yes\n"
          },
          {
            "role": "user",
            "content": "Question:\nμ…λ ¥μΌλ΅ μ κ³µλ μƒν’ ν•­λ©μ΄ ν‘ μ•μ— ν¬ν•¨λμ–΄ μμµλ‹κΉ?\n"
          },
          {
            "role": "assistant",
            "content": "Yes\n"
          },
          {
            "role": "user",
            "content": "Question:\nμƒμ„±λ ν‘ μ•μ λ¨λ“  μƒν’λ“¤μ€ μ…λ ¥μΌλ΅ μ£Όμ–΄μ§„ μƒν’κ³Ό κ°™μ€ μ ν•μ μ ν’μ…λ‹κΉ?\n"
          },
          {
            "role": "assistant",
            "content": "Yes\n"
          },
          {
            "role": "user",
            "content": "Question:\nν‘ μ•μ— μ‘μ„±λ μƒν’ ν•­λ©μ νΉμ§•λ“¤μ΄ κ°€κ²©, λ³„μ , λ‚΄μ§€ ν•νƒ, μΆ…μ΄ μ‚¬μ΄μ¦ κ° μ»¬λΌμ— μ•λ§λ” λ‚΄μ©μΌλ΅ μ‘μ„±λμ—μµλ‹κΉ?\n"
          },
          {
            "role": "assistant",
            "content": "No\n"
          }
        ],
        "answers": [
          "Yes",
          "Yes",
          "Yes",
          "No"
        ]
      },
      ...
    ]
  }
}
```
</pre>
</details>
<br>
Please note that the results of an automated evaluation using GPT-4 may differ from the results of an in-person assessment by human experts. In our setting, the degree of alignment between human experts and llm judgement is about **83%**.

## Evaluation Result (ours)

### DRFR
Decomposed Requirements Following Ratio(DRFR) is the metric to evaluate how LLMs accurately respond to the instruction/input.
This metric calculates the average accuracy across answers to the decomposed questions for each instruction.
The following is the summary of the model performance on our dataset.

| Model                                       | H_DRFR     | A_DRFR | Alignment |
|---------------------------------------------|------------|--------|-----------|
| **claude-3-opus-20240229**                  | **0.854**  | 0.850  | 87%       |
| **gpt-4-turbo-2024-04-09**                  | 0.850      | 0.880  | 87%       |
| **gpt-4-0125-preview**                      | 0.824      | 0.824  | 83%       |
| **gemini-1.5-pro**                          | 0.773      | 0.811  | 83%       |
| **meta-llama/Meta-Llama-3-70B-Instruct**   | 0.747      | 0.863  | 84%       |
| **hpx003**                                  | 0.691      | 0.738  | 83%       |
| **gpt-3.5-turbo-0125**                      | 0.678      | 0.734  | 82%       |
| **yanolja/EEVE-Korean-Instruct-10.8B-v1.0** | 0.597      | 0.730  | 79%       |

- `H_DRFR`: The accuracy of model responses as evaluated by the human expert
- `A_DRFR`: The accuracy of model responses automatically evaluated by GPT-4 as employing the capability of LLM-as-a-judge
- `Alignment`: The degree of agreement or consistency between the human and automated evaluation

## Additional Information

### License Information

This dataset is released under the [MIT LISENCE](https://github.com/KIFAI/KoInfoBench/blob/main/LICENSE)

### Citation
```
@article{,
      title={KoInFoBench: Evaluations for Korean Insturction-Following Capabilities of LLM}, 
      author={Sungwoo Oh, Sungjun Kown, Donggyu Kim},
      year={2024},
      eprint={},
}
```
